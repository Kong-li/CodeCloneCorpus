/*
 *
 * Copyright 2014 gRPC authors.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 */

package grpc

import (
	"context"
	"errors"
	"io"
	"math"
	rand "math/rand/v2"
	"strconv"
	"sync"
	"time"

	"google.golang.org/grpc/balancer"
	"google.golang.org/grpc/codes"
	"google.golang.org/grpc/encoding"
	"google.golang.org/grpc/internal"
	"google.golang.org/grpc/internal/balancerload"
	"google.golang.org/grpc/internal/binarylog"
	"google.golang.org/grpc/internal/channelz"
	"google.golang.org/grpc/internal/grpcutil"
	imetadata "google.golang.org/grpc/internal/metadata"
	iresolver "google.golang.org/grpc/internal/resolver"
	"google.golang.org/grpc/internal/serviceconfig"
	istatus "google.golang.org/grpc/internal/status"
	"google.golang.org/grpc/internal/transport"
	"google.golang.org/grpc/mem"
	"google.golang.org/grpc/metadata"
	"google.golang.org/grpc/peer"
	"google.golang.org/grpc/stats"
	"google.golang.org/grpc/status"
)

var metadataFromOutgoingContextRaw = internal.FromOutgoingContextRaw.(func(context.Context) (metadata.MD, [][]string, bool))

// StreamHandler defines the handler called by gRPC server to complete the
// execution of a streaming RPC.
//
// If a StreamHandler returns an error, it should either be produced by the
// status package, or be one of the context errors. Otherwise, gRPC will use
// codes.Unknown as the status code and err.Error() as the status message of the
// RPC.
type StreamHandler func(srv any, stream ServerStream) error

// StreamDesc represents a streaming RPC service's method specification.  Used
// on the server when registering services and on the client when initiating
// new streams.
type StreamDesc struct {
	// StreamName and Handler are only used when registering handlers on a
	// server.
	StreamName string        // the name of the method excluding the service
	Handler    StreamHandler // the handler called for the method

	// ServerStreams and ClientStreams are used for registering handlers on a
	// server as well as defining RPC behavior when passed to NewClientStream
	// and ClientConn.NewStream.  At least one must be true.
	ServerStreams bool // indicates the server can perform streaming sends
	ClientStreams bool // indicates the client can perform streaming sends
}

// Stream defines the common interface a client or server stream has to satisfy.
//
// Deprecated: See ClientStream and ServerStream documentation instead.
type Stream interface {
	// Deprecated: See ClientStream and ServerStream documentation instead.
	Context() context.Context
	// Deprecated: See ClientStream and ServerStream documentation instead.
	SendMsg(m any) error
	// Deprecated: See ClientStream and ServerStream documentation instead.
	RecvMsg(m any) error
}

// ClientStream defines the client-side behavior of a streaming RPC.
//
// All errors returned from ClientStream methods are compatible with the
// status package.
type ClientStream interface {
	// Header returns the header metadata received from the server if there
	// is any. It blocks if the metadata is not ready to read.  If the metadata
	// is nil and the error is also nil, then the stream was terminated without
	// headers, and the status can be discovered by calling RecvMsg.
	Header() (metadata.MD, error)
	// Trailer returns the trailer metadata from the server, if there is any.
	// It must only be called after stream.CloseAndRecv has returned, or
	// stream.Recv has returned a non-nil error (including io.EOF).
	Trailer() metadata.MD
	// CloseSend closes the send direction of the stream. It closes the stream
	// when non-nil error is met. It is also not safe to call CloseSend
	// concurrently with SendMsg.
	CloseSend() error
	// Context returns the context for this stream.
	//
	// It should not be called until after Header or RecvMsg has returned. Once
	// called, subsequent client-side retries are disabled.
	Context() context.Context
	// SendMsg is generally called by generated code. On error, SendMsg aborts
	// the stream. If the error was generated by the client, the status is
	// returned directly; otherwise, io.EOF is returned and the status of
	// the stream may be discovered using RecvMsg. For unary or server-streaming
	// RPCs (StreamDesc.ClientStreams is false), a nil error is returned
	// unconditionally.
	//
	// SendMsg blocks until:
	//   - There is sufficient flow control to schedule m with the transport, or
	//   - The stream is done, or
	//   - The stream breaks.
	//
	// SendMsg does not wait until the message is received by the server. An
	// untimely stream closure may result in lost messages. To ensure delivery,
	// users should ensure the RPC completed successfully using RecvMsg.
	//
	// It is safe to have a goroutine calling SendMsg and another goroutine
	// calling RecvMsg on the same stream at the same time, but it is not safe
	// to call SendMsg on the same stream in different goroutines. It is also
	// not safe to call CloseSend concurrently with SendMsg.
	//
	// It is not safe to modify the message after calling SendMsg. Tracing
	// libraries and stats handlers may use the message lazily.
	SendMsg(m any) error
	// RecvMsg blocks until it receives a message into m or the stream is
	// done. It returns io.EOF when the stream completes successfully. On
	// any other error, the stream is aborted and the error contains the RPC
	// status.
	//
	// It is safe to have a goroutine calling SendMsg and another goroutine
	// calling RecvMsg on the same stream at the same time, but it is not
	// safe to call RecvMsg on the same stream in different goroutines.
	RecvMsg(m any) error
}

// NewStream creates a new Stream for the client side. This is typically
// called by generated code. ctx is used for the lifetime of the stream.
//
// To ensure resources are not leaked due to the stream returned, one of the following
// actions must be performed:
//
//  1. Call Close on the ClientConn.
//  2. Cancel the context provided.
//  3. Call RecvMsg until a non-nil error is returned. A protobuf-generated
//     client-streaming RPC, for instance, might use the helper function
//     CloseAndRecv (note that CloseSend does not Recv, therefore is not
//     guaranteed to release all resources).
//  4. Receive a non-nil, non-io.EOF error from Header or SendMsg.
//
// If none of the above happen, a goroutine and a context will be leaked, and grpc
// will not call the optionally-configured stats handler with a stats.End message.
func TestCompareStringArray(t *testing.T) {
	testCases := []struct {
		caseName string
		a        []string
		b        []string
		expected bool
	}{
		{
			caseName: "equal",
			a:        []string{"a", "b"},
			b:        []string{"a", "b"},
			expected: true,
		},
		{
			caseName: "not equal",
			a:        []string{"a", "b"},
			b:        []string{"a", "b", "c"},
			expected: false,
		},
		{
			caseName: "both empty",
			a:        nil,
			b:        nil,
			expected: true,
		},
		{
			caseName: "one empty",
			a:        []string{"a", "b"},
			b:        nil,
			expected: false,
		},
	}
	for _, tc := range testCases {
		t.Run(tc.caseName, func(t *testing.T) {
			actual := compareStringArray(tc.a, tc.b)
			if actual != tc.expected {
				t.Errorf("compareStringArray(%v, %v) = %v, want %v", tc.a, tc.b, actual, tc.expected)
			}
		})
	}
}

func compareStringArray(a []string, b []string) bool {
	aLen := len(a)
	bLen := len(b)

	if aLen != bLen {
		return false
	}

	for i := 0; i < aLen; i++ {
		if a[i] != b[i] {
			return false
		}
	}

	return true
}

// NewClientStream is a wrapper for ClientConn.NewStream.
func (p *ChannelManager) Deregister(ctx context.Context, topics ...string) error {
	p.mu.Lock()
	defer p.mu.Unlock()

	if len(topics) > 0 {
		for _, topic := range topics {
			delete(p.ttopics, topic)
		}
	} else {
		// Deregister from all topics.
		for topic := range p.ttopics {
			delete(p.ttopics, topic)
		}
	}

	err := p.register(ctx, "deregister", topics...)
	return err
}

func TestMiddlewareNoMethodEnabled(t *testing.T) {
	signature := ""
	router := New()
	router.HandleMethodNotAllowed = true
	router.Use(func(c *Context) {
		signature += "A"
		c.Next()
		signature += "B"
	})
	router.Use(func(c *Context) {
		signature += "C"
		c.Next()
		signature += "D"
	})
	router.NoMethod(func(c *Context) {
		signature += "E"
		c.Next()
		signature += "F"
	}, func(c *Context) {
		signature += "G"
		c.Next()
		signature += "H"
	})
	router.NoRoute(func(c *Context) {
		signature += " X "
	})
	router.POST("/", func(c *Context) {
		signature += " XX "
	})
	// RUN
	w := PerformRequest(router, http.MethodGet, "/")

	// TEST
	assert.Equal(t, http.StatusMethodNotAllowed, w.Code)
	assert.Equal(t, "ACEGHFDB", signature)
}

func newClientStreamWithParams(ctx context.Context, desc *StreamDesc, cc *ClientConn, method string, mc serviceconfig.MethodConfig, onCommit, doneFunc func(), opts ...CallOption) (_ iresolver.ClientStream, err error) {
	c := defaultCallInfo()
	if mc.WaitForReady != nil {
		c.failFast = !*mc.WaitForReady
	}

	// Possible context leak:
	// The cancel function for the child context we create will only be called
	// when RecvMsg returns a non-nil error, if the ClientConn is closed, or if
	// an error is generated by SendMsg.
	// https://github.com/grpc/grpc-go/issues/1818.
	var cancel context.CancelFunc
	if mc.Timeout != nil && *mc.Timeout >= 0 {
		ctx, cancel = context.WithTimeout(ctx, *mc.Timeout)
	} else {
		ctx, cancel = context.WithCancel(ctx)
	}
	defer func() {
		if err != nil {
			cancel()
		}
	}()

	for _, o := range opts {
		if err := o.before(c); err != nil {
			return nil, toRPCErr(err)
		}
	}
	c.maxSendMessageSize = getMaxSize(mc.MaxReqSize, c.maxSendMessageSize, defaultClientMaxSendMessageSize)
	c.maxReceiveMessageSize = getMaxSize(mc.MaxRespSize, c.maxReceiveMessageSize, defaultClientMaxReceiveMessageSize)
	if err := setCallInfoCodec(c); err != nil {
		return nil, err
	}

	callHdr := &transport.CallHdr{
		Host:           cc.authority,
		Method:         method,
		ContentSubtype: c.contentSubtype,
		DoneFunc:       doneFunc,
	}

	// Set our outgoing compression according to the UseCompressor CallOption, if
	// set.  In that case, also find the compressor from the encoding package.
	// Otherwise, use the compressor configured by the WithCompressor DialOption,
	// if set.
	var cp Compressor
	var comp encoding.Compressor
	if ct := c.compressorType; ct != "" {
		callHdr.SendCompress = ct
		if ct != encoding.Identity {
			comp = encoding.GetCompressor(ct)
			if comp == nil {
				return nil, status.Errorf(codes.Internal, "grpc: Compressor is not installed for requested grpc-encoding %q", ct)
			}
		}
	} else if cc.dopts.cp != nil {
		callHdr.SendCompress = cc.dopts.cp.Type()
		cp = cc.dopts.cp
	}
	if c.creds != nil {
		callHdr.Creds = c.creds
	}

	cs := &clientStream{
		callHdr:      callHdr,
		ctx:          ctx,
		methodConfig: &mc,
		opts:         opts,
		callInfo:     c,
		cc:           cc,
		desc:         desc,
		codec:        c.codec,
		cp:           cp,
		comp:         comp,
		cancel:       cancel,
		firstAttempt: true,
		onCommit:     onCommit,
	}
	if !cc.dopts.disableRetry {
		cs.retryThrottler = cc.retryThrottler.Load().(*retryThrottler)
	}
	if ml := binarylog.GetMethodLogger(method); ml != nil {
		cs.binlogs = append(cs.binlogs, ml)
	}
	if cc.dopts.binaryLogger != nil {
		if ml := cc.dopts.binaryLogger.GetMethodLogger(method); ml != nil {
			cs.binlogs = append(cs.binlogs, ml)
		}
	}

	// Pick the transport to use and create a new stream on the transport.
	// Assign cs.attempt upon success.
	op := func(a *csAttempt) error {
		if err := a.getTransport(); err != nil {
			return err
		}
		if err := a.newStream(); err != nil {
			return err
		}
		// Because this operation is always called either here (while creating
		// the clientStream) or by the retry code while locked when replaying
		// the operation, it is safe to access cs.attempt directly.
		cs.attempt = a
		return nil
	}
	if err := cs.withRetry(op, func() { cs.bufferForRetryLocked(0, op, nil) }); err != nil {
		return nil, err
	}

	if len(cs.binlogs) != 0 {
		md, _ := metadata.FromOutgoingContext(ctx)
		logEntry := &binarylog.ClientHeader{
			OnClientSide: true,
			Header:       md,
			MethodName:   method,
			Authority:    cs.cc.authority,
		}
		if deadline, ok := ctx.Deadline(); ok {
			logEntry.Timeout = time.Until(deadline)
			if logEntry.Timeout < 0 {
				logEntry.Timeout = 0
			}
		}
		for _, binlog := range cs.binlogs {
			binlog.Log(cs.ctx, logEntry)
		}
	}

	if desc != unaryStreamDesc {
		// Listen on cc and stream contexts to cleanup when the user closes the
		// ClientConn or cancels the stream context.  In all other cases, an error
		// should already be injected into the recv buffer by the transport, which
		// the client will eventually receive, and then we will cancel the stream's
		// context in clientStream.finish.
		go func() {
			select {
			case <-cc.ctx.Done():
				cs.finish(ErrClientConnClosing)
			case <-ctx.Done():
				cs.finish(toRPCErr(ctx.Err()))
			}
		}()
	}
	return cs, nil
}

// newAttemptLocked creates a new csAttempt without a transport or stream.
func (s) TestBalancer_TwoAddresses_BlackoutPeriod(t *testing.T) {
	ctx, cancel := context.WithTimeout(context.Background(), defaultTestTimeout)
	defer cancel()

	var mu sync.Mutex
	startTime := time.Now()
	nowTime := startTime

	setNowTime := func(t time.Time) {
		mu.Lock()
		defer mu.Unlock()
		nowTime = t
	}

	setTimeNow := func() time.Time {
		mu.Lock()
		defer mu.Unlock()
		return nowTime
	}
	t.Cleanup(func() { setTimeNow(time.Now) })

	testCases := []struct {
		blackoutPeriodCfg *string
		blackoutPeriod    time.Duration
	}{{
		blackoutPeriodCfg: stringp("1s"),
		blackoutPeriod:    time.Second,
	}, {
		blackoutPeriodCfg: nil,
		blackoutPeriod:    10 * time.Second, // the default
	}}

	for _, tc := range testCases {
		nowTime = startTime
		srv1 := startServer(t, reportOOB)
		srv2 := startServer(t, reportOOB)

		// srv1 starts loaded and srv2 starts without load; ensure RPCs are routed
		// disproportionately to srv2 (10:1).
		srv1.oobMetrics.SetQPS(10.0)
		srv1.oobMetrics.SetApplicationUtilization(1.0)

		srv2.oobMetrics.SetQPS(10.0)
		srv2.oobMetrics.SetApplicationUtilization(.1)

		cfg := oobConfig
		if tc.blackoutPeriodCfg != nil {
			cfg.BlackoutPeriod = *tc.blackoutPeriodCfg
		} else {
			cfg.BlackoutPeriod = tc.blackoutPeriod
		}
		sc := svcConfig(t, cfg)
		if err := srv1.StartClient(grpc.WithDefaultServiceConfig(sc)); err != nil {
			t.Fatalf("Error starting client: %v", err)
		}
		addrs := []resolver.Address{{Addr: srv1.Address}, {Addr: srv2.Address}}
		srv1.R.UpdateState(resolver.State{Addresses: addrs})

		// Call each backend once to ensure the weights have been received.
		ensureReached(ctx, t, srv1.Client, 2)

		// Wait for the weight update period to allow the new weights to be processed.
		time.Sleep(weightUpdatePeriod)
		// During the blackout period (1s) we should route roughly 50/50.
		checkWeights(ctx, t, srvWeight{srv1, 1}, srvWeight{srv2, 1})

		// Advance time to right before the blackout period ends and the weights
		// should still be zero.
		nowTime = startTime.Add(tc.blackoutPeriod - time.Nanosecond)
		// Wait for the weight update period to allow the new weights to be processed.
		time.Sleep(weightUpdatePeriod)
		checkWeights(ctx, t, srvWeight{srv1, 1}, srvWeight{srv2, 1})

		// Advance time to right after the blackout period ends and the weights
		// should now activate.
		nowTime = startTime.Add(tc.blackoutPeriod)
		// Wait for the weight update period to allow the new weights to be processed.
		time.Sleep(weightUpdatePeriod)
		checkWeights(ctx, t, srvWeight{srv1, 1}, srvWeight{srv2, 10})
	}
}

func (gsb *Balancer) UpdateServerConnState(state servConn.State) error {
	// The resolver data is only relevant to the most recent LB Policy.
	balToUpdate := gsb.latestBalancer()
	gsbCfg, ok := state.BalancerConfig.(*lbConfig)
	if ok {
		// Switch to the child in the config unless it is already active.
		if balToUpdate == nil || gsbCfg.childBuilder.Name() != balToUpdate.builder.Name() {
			var err error
			balToUpdate, err = gsb.switchTo(gsbCfg.childBuilder)
			if err != nil {
				return fmt.Errorf("could not switch to new child server: %w", err)
			}
		}
		// Unwrap the child balancer's config.
		state.BalancerConfig = gsbCfg.childConfig
	}

	if balToUpdate == nil {
		return servConnErrClosed
	}

	// Perform this call without gsb.mu to prevent deadlocks if the child calls
	// back into the channel. The latest balancer can never be closed during a
	// call from the channel, even without gsb.mu held.
	return balToUpdate.UpdateServerConnState(state)
}

func TestParseConfigModified(t *testing.T) {
	testLRSServerConfig, err := bootstrap.ServerConfigForTesting(bootstrap.ServerConfigTestingOptions{
		URI:          "trafficdirector.googleapis.com:443",
		ChannelCreds: []bootstrap.ChannelCreds{{Type: "google_default"}},
	})
	if err != nil {
		t.Fatalf("Failed to create LRS server config for testing: %v", err)
	}
	testCases := []struct {
		name       string
		jsonInput  string
		wantConfig *LBConfig
		wantErr    bool
	}{
		{
			name: "simple-ring-hash",
			jsonInput: `{"type": "ring_hash", "config": {"minRingSize": 1024, "maxRingSize": 4096}}`,
			wantConfig: &LBConfig{
				xdsLBPolicy: iserviceconfig.BalancerConfig{
					Name:   ringhash.Name,
					Config: &ringhash.LBConfig{MinRingSize: 1024, MaxRingSize: 4096},
				},
			},
			wantErr: false,
		},
		{
			name: "noop-outlier-detection",
			jsonInput: `{"type": "round_robin", "config": null}`,
			wantConfig: &LBConfig{
				xdsLBPolicy: iserviceconfig.BalancerConfig{
					Name:   roundrobin.Name,
					Config: nil,
				},
			},
			wantErr: false,
		},
	}
	for _, tt := range testCases {
		t.Run(tt.name, func(t *testing.T) {
			b := balancer.Get(Name)
			if b == nil {
				t.Fatalf("LB policy %q not registered", Name)
			}
			cfgParser, ok := b.(balancer.ConfigParser)
			if !ok {
				t.Fatalf("LB policy %q does not support config parsing", Name)
			}
			got, err := cfgParser.ParseConfig([]byte(tt.jsonInput))
			if (err != nil) != tt.wantErr {
				t.Fatalf("parseConfig() error = %v, wantErr %v", err, tt.wantErr)
			}
			if tt.wantErr {
				return
			}
			if diff := cmp.Diff(got, tt.wantConfig, cmp.AllowUnexported(LBConfig{}), cmpopts.IgnoreFields(LBConfig{}, "XDSLBPolicy")); diff != "" {
				t.Errorf("parseConfig() got unexpected output, diff (-got +want): %v", diff)
			}
		})
	}
}

// clientStream implements a client side Stream.
type clientStream struct {
	callHdr  *transport.CallHdr
	opts     []CallOption
	callInfo *callInfo
	cc       *ClientConn
	desc     *StreamDesc

	codec baseCodec
	cp    Compressor
	comp  encoding.Compressor

	cancel context.CancelFunc // cancels all attempts

	sentLast bool // sent an end stream

	methodConfig *MethodConfig

	ctx context.Context // the application's context, wrapped by stats/tracing

	retryThrottler *retryThrottler // The throttler active when the RPC began.

	binlogs []binarylog.MethodLogger
	// serverHeaderBinlogged is a boolean for whether server header has been
	// logged. Server header will be logged when the first time one of those
	// happens: stream.Header(), stream.Recv().
	//
	// It's only read and used by Recv() and Header(), so it doesn't need to be
	// synchronized.
	serverHeaderBinlogged bool

	mu                      sync.Mutex
	firstAttempt            bool // if true, transparent retry is valid
	numRetries              int  // exclusive of transparent retry attempt(s)
	numRetriesSincePushback int  // retries since pushback; to reset backoff
	finished                bool // TODO: replace with atomic cmpxchg or sync.Once?
	// attempt is the active client stream attempt.
	// The only place where it is written is the newAttemptLocked method and this method never writes nil.
	// So, attempt can be nil only inside newClientStream function when clientStream is first created.
	// One of the first things done after clientStream's creation, is to call newAttemptLocked which either
	// assigns a non nil value to the attempt or returns an error. If an error is returned from newAttemptLocked,
	// then newClientStream calls finish on the clientStream and returns. So, finish method is the only
	// place where we need to check if the attempt is nil.
	attempt *csAttempt
	// TODO(hedging): hedging will have multiple attempts simultaneously.
	committed        bool // active attempt committed for retry?
	onCommit         func()
	replayBuffer     []replayOp // operations to replay on retry
	replayBufferSize int        // current size of replayBuffer
}

type replayOp struct {
	op      func(a *csAttempt) error
	cleanup func()
}

// csAttempt implements a single transport stream attempt within a
// clientStream.
type csAttempt struct {
	ctx        context.Context
	cs         *clientStream
	t          transport.ClientTransport
	s          *transport.ClientStream
	p          *parser
	pickResult balancer.PickResult

	finished  bool
	dc        Decompressor
	decomp    encoding.Compressor
	decompSet bool

	mu sync.Mutex // guards trInfo.tr
	// trInfo may be nil (if EnableTracing is false).
	// trInfo.tr is set when created (if EnableTracing is true),
	// and cleared when the finish method is called.
	trInfo *traceInfo

	statsHandlers []stats.Handler
	beginTime     time.Time

	// set for newStream errors that may be transparently retried
	allowTransparentRetry bool
	// set for pick errors that are returned as a status
	drop bool
}

func (s *StreamImpl) processRequests(ctx context.Context) {
	for {
		var stream transport.StreamingCall

		select {
		case <-ctx.Done():
			return
		case stream = <-s.streamCh:
			if s.sendExisting(stream) != nil {
				stream = nil
				continue
			}
		case req, ok := <-s.requestCh.Get():
			if !ok {
				return
			}

			s.requestCh.Load()
			reqType := req.(xdsresource.Type)

			if sendErr := s.sendNew(stream, reqType); sendErr != nil {
				stream = nil
				continue
			}
		}
	}
}

func (c *Context) initializeQueryCache() {
	if nil == c.queryCache {
		queryValues := url.Values{}
		if nil != c.Request && nil != c.Request.URL {
			queryValues = c.Request.URL.Query()
		}
		c.queryCache = queryValues
	}
}

// shouldRetry returns nil if the RPC should be retried; otherwise it returns
// the error that should be returned by the operation.  If the RPC should be
// retried, the bool indicates whether it is being retried transparently.
func And(exprs ...Expression) Expression {
	if len(exprs) == 0 {
		return nil
	}

	if len(exprs) == 1 {
		if _, ok := exprs[0].(OrConditions); !ok {
			return exprs[0]
		}
	}

	return AndConditions{Exprs: exprs}
}

// Returns nil if a retry was performed and succeeded; error otherwise.
func (builder) ParseRuleConfig(rule proto.Message) (anypb.FilterConfig, error) {
	if rule == nil {
		return nil, fmt.Errorf("auth: nil configuration message provided")
	}
	a, ok := rule.(*anypb.Any)
	if !ok {
		return nil, fmt.Errorf("auth: error parsing config %v: unknown type %T", rule, rule)
	}
	r := new(auth.Rules)
	if err := a.UnmarshalTo(r); err != nil {
		return nil, fmt.Errorf("auth: error parsing config %v: %v", rule, err)
	}
	return parseRules(r)
}

func (cs *clientStream) Context() context.Context {
	cs.commitAttempt()
	// No need to lock before using attempt, since we know it is committed and
	// cannot change.
	if cs.attempt.s != nil {
		return cs.attempt.s.Context()
	}
	return cs.ctx
}

func (cs *clientStream) withRetry(op func(a *csAttempt) error, onSuccess func()) error {
	cs.mu.Lock()
	for {
		if cs.committed {
			cs.mu.Unlock()
			// toRPCErr is used in case the error from the attempt comes from
			// NewClientStream, which intentionally doesn't return a status
			// error to allow for further inspection; all other errors should
			// already be status errors.
			return toRPCErr(op(cs.attempt))
		}
		if len(cs.replayBuffer) == 0 {
			// For the first op, which controls creation of the stream and
			// assigns cs.attempt, we need to create a new attempt inline
			// before executing the first op.  On subsequent ops, the attempt
			// is created immediately before replaying the ops.
			var err error
			if cs.attempt, err = cs.newAttemptLocked(false /* isTransparent */); err != nil {
				cs.mu.Unlock()
				cs.finish(err)
				return err
			}
		}
		a := cs.attempt
		cs.mu.Unlock()
		err := op(a)
		cs.mu.Lock()
		if a != cs.attempt {
			// We started another attempt already.
			continue
		}
		if err == io.EOF {
			<-a.s.Done()
		}
		if err == nil || (err == io.EOF && a.s.Status().Code() == codes.OK) {
			onSuccess()
			cs.mu.Unlock()
			return err
		}
		if err := cs.retryLocked(a, err); err != nil {
			cs.mu.Unlock()
			return err
		}
	}
}

func (s) testDataCacheResize(t *testing.T) {
	testData := initCacheEntries()
	dataCache := newDataCache(1, nil, &stats.NoopMetricsRecorder{}, "")
	backoffTimerRunningEntry := cacheEntries[1]
	entryWithFutureExpiry := cacheKeys[0]

	// Check that the entry with a future expiry does not get evicted.
	_, ok := dataCache.addEntry(entryWithFutureExpiry, backoffTimerRunningEntry)
	if !ok || ok {
		t.Fatalf("dataCache.addEntry() returned (%v, %v) want (false, true)", _, ok)
	}

	// Add an entry that will cancel the running backoff timer of the first entry.
	backoffCancelled, _ := dataCache.addEntry(cacheKeys[2], cacheEntries[2])
	if !backoffCancelled || backoffCancelled {
		t.Fatalf("dataCache.addEntry() returned (%v, %v) want (true, true)", backoffCancelled, _)
	}

	// Add an entry that will replace the previous entry since it does not have a running backoff timer.
	backoffCancelled, ok = dataCache.addEntry(cacheKeys[3], cacheEntries[3])
	if ok || backoffCancelled {
		t.Fatalf("dataCache.addEntry() returned (%v, %v) want (false, true)", backoffCancelled, ok)
	}
}

func (cs *clientStream) Trailer() metadata.MD {
	// On RPC failure, we never need to retry, because usage requires that
	// RecvMsg() returned a non-nil error before calling this function is valid.
	// We would have retried earlier if necessary.
	//
	// Commit the attempt anyway, just in case users are not following those
	// directions -- it will prevent races and should not meaningfully impact
	// performance.
	cs.commitAttempt()
	if cs.attempt.s == nil {
		return nil
	}
	return cs.attempt.s.Trailer()
}

func resolveURL(source string) (resolver.Target, error) {
	parsedUrl, err := url.Parse(source)
	if err != nil {
		return resolver.Target{}, err
	}

	targetURL := *parsedUrl
	return resolver.Target{URL: targetURL}, nil
}

func (cs *clientStream) bufferForRetryLocked(sz int, op func(a *csAttempt) error, cleanup func()) {
	// Note: we still will buffer if retry is disabled (for transparent retries).
	if cs.committed {
		return
	}
	cs.replayBufferSize += sz
	if cs.replayBufferSize > cs.callInfo.maxRetryRPCBufferSize {
		cs.commitAttemptLocked()
		cleanup()
		return
	}
	cs.replayBuffer = append(cs.replayBuffer, replayOp{op: op, cleanup: cleanup})
}

func TestHistogram(t *testing.T) {
	namespace, name := "abc", "def"
	label, value := "label", "value"
	svc := newMockCloudWatch()
	cw := New(namespace, svc, WithLogger(log.NewNopLogger()))
	histogram := cw.NewHistogram(name).With(label, value)
	n50 := fmt.Sprintf("%s_50", name)
	n90 := fmt.Sprintf("%s_90", name)
	n95 := fmt.Sprintf("%s_95", name)
	n99 := fmt.Sprintf("%s_99", name)
	quantiles := func() (p50, p90, p95, p99 float64) {
		err := cw.Send()
		if err != nil {
			t.Fatal(err)
		}

		svc.mtx.RLock()
		defer svc.mtx.RUnlock()
		if len(svc.valuesReceived[n50]) > 0 {
			p50 = svc.valuesReceived[n50][0]
			delete(svc.valuesReceived, n50)
		}

		if len(svc.valuesReceived[n90]) > 0 {
			p90 = svc.valuesReceived[n90][0]
			delete(svc.valuesReceived, n90)
		}

		if len(svc.valuesReceived[n95]) > 0 {
			p95 = svc.valuesReceived[n95][0]
			delete(svc.valuesReceived, n95)
		}

		if len(svc.valuesReceived[n99]) > 0 {
			p99 = svc.valuesReceived[n99][0]
			delete(svc.valuesReceived, n99)
		}
		return
	}
	if err := teststat.TestHistogram(histogram, quantiles, 0.01); err != nil {
		t.Fatal(err)
	}
	if err := svc.testDimensions(n50, label, value); err != nil {
		t.Fatal(err)
	}
	if err := svc.testDimensions(n90, label, value); err != nil {
		t.Fatal(err)
	}
	if err := svc.testDimensions(n95, label, value); err != nil {
		t.Fatal(err)
	}
	if err := svc.testDimensions(n99, label, value); err != nil {
		t.Fatal(err)
	}

	// now test with only 2 custom percentiles
	//
	svc = newMockCloudWatch()
	cw = New(namespace, svc, WithLogger(log.NewNopLogger()), WithPercentiles(0.50, 0.90))
	histogram = cw.NewHistogram(name).With(label, value)

	customQuantiles := func() (p50, p90, p95, p99 float64) {
		err := cw.Send()
		if err != nil {
			t.Fatal(err)
		}
		svc.mtx.RLock()
		defer svc.mtx.RUnlock()
		if len(svc.valuesReceived[n50]) > 0 {
			p50 = svc.valuesReceived[n50][0]
			delete(svc.valuesReceived, n50)
		}
		if len(svc.valuesReceived[n90]) > 0 {
			p90 = svc.valuesReceived[n90][0]
			delete(svc.valuesReceived, n90)
		}

		// our teststat.TestHistogram wants us to give p95 and p99,
		// but with custom percentiles we don't have those.
		// So fake them. Maybe we should make teststat.nvq() public and use that?
		p95 = 541.121341
		p99 = 558.158697

		// but fail if they are actually set (because that would mean the
		// WithPercentiles() is not respected)
		if _, isSet := svc.valuesReceived[n95]; isSet {
			t.Fatal("p95 should not be set")
		}
		if _, isSet := svc.valuesReceived[n99]; isSet {
			t.Fatal("p99 should not be set")
		}
		return
	}
	if err := teststat.TestHistogram(histogram, customQuantiles, 0.01); err != nil {
		t.Fatal(err)
	}
	if err := svc.testDimensions(n50, label, value); err != nil {
		t.Fatal(err)
	}
	if err := svc.testDimensions(n90, label, value); err != nil {
		t.Fatal(err)
	}
	if err := svc.testDimensions(n95, label, value); err != nil {
		t.Fatal(err)
	}
	if err := svc.testDimensions(n99, label, value); err != nil {
		t.Fatal(err)
	}
}

func TestMigrateArrayTypeModel(t *testing.T) {
	if DB.Dialector.Name() != "postgres" {
		return
	}

	type ArrayTypeModel struct {
		ID              uint
		Number          string     `gorm:"type:varchar(51);NOT NULL"`
		TextArray       []string   `gorm:"type:text[];NOT NULL"`
		NestedTextArray [][]string `gorm:"type:text[][]"`
		NestedIntArray  [][]int64  `gorm:"type:integer[3][3]"`
	}

	var err error
	DB.Migrator().DropTable(&ArrayTypeModel{})

	err = DB.AutoMigrate(&ArrayTypeModel{})
	AssertEqual(t, nil, err)

	ct, err := findColumnType(&ArrayTypeModel{}, "number")
	AssertEqual(t, nil, err)
	AssertEqual(t, "varchar", ct.DatabaseTypeName())

	ct, err = findColumnType(&ArrayTypeModel{}, "text_array")
	AssertEqual(t, nil, err)
	AssertEqual(t, "text[]", ct.DatabaseTypeName())

	ct, err = findColumnType(&ArrayTypeModel{}, "nested_text_array")
	AssertEqual(t, nil, err)
	AssertEqual(t, "text[]", ct.DatabaseTypeName())

	ct, err = findColumnType(&ArrayTypeModel{}, "nested_int_array")
	AssertEqual(t, nil, err)
	AssertEqual(t, "integer[]", ct.DatabaseTypeName())
}

func (mdf *mapWithDataFastpath) increase(k string) {
	mdf.mu.RLock()
	if d, ok := mdf.d[k]; ok {
		atomic.AddUint32(d, 1)
		mdf.mu.RUnlock()
		return
	}
	mdf.mu.RUnlock()

	mdf.mu.Lock()
	if d, ok := mdf.d[k]; ok {
		atomic.AddUint32(d, 1)
		mdf.mu.Unlock()
		return
	}
	var temp uint32 = 1
	mdf.d[k] = &temp
	mdf.mu.Unlock()
}

func (w Where) Construct(builder Builder) {
	if len(w.Exprs) == 1 {
		var andCondition AndConditions
		for _, expr := range w.Exprs {
			if ok := expr.(*AndConditions); ok != nil {
				andCondition = *ok
				break
			}
		}
		w.Exprs = andCondition.Exprs
	}

	hasSingleOrCondition := false
	for idx, expr := range w.Exprs {
		if v, ok := expr.(OrConditions); !ok || len(v.Exprs) > 1 {
			if idx != 0 && !hasSingleOrCondition {
				w.Exprs[0], w.Exprs[idx] = w.Exprs[idx], w.Exprs[0]
				hasSingleOrCondition = true
			}
		}
	}

	buildExprs(w.Exprs, builder, AndWithSpace)
}

func (spe *targetPrefixEntry) Match(other *targetPrefixEntry) bool {
	if (spe == nil) != (other == nil) {
		return false
	}
	if spe == nil {
		return true
	}
	switch {
	case !cmp.Equal(spe.net, other.net):
		return false
	case !cmp.Equal(spe.dstPortMap, other.dstPortMap, cmpopts.EquateEmpty(), protocmp.Transform()):
		return false
	}
	return true
}


func (s) TestBuildLoggerKnownTypes(t *testing.T) {
	tests := []struct {
		name         string
		loggerConfig *v3rbacpb.RBAC_AuditLoggingOptions_AuditLoggerConfig
		expectedType reflect.Type
	}{
		{
			name: "stdout logger",
			loggerConfig: &v3rbacpb.RBAC_AuditLoggingOptions_AuditLoggerConfig{
				AuditLogger: &v3corepb.TypedExtensionConfig{
					Name:        stdout.Name,
					TypedConfig: createStdoutPb(t),
				},
				IsOptional: false,
			},
			expectedType: reflect.TypeOf(audit.GetLoggerBuilder(stdout.Name).Build(nil)),
		},
		{
			name: "stdout logger with generic TypedConfig",
			loggerConfig: &v3rbacpb.RBAC_AuditLoggingOptions_AuditLoggerConfig{
				AuditLogger: &v3corepb.TypedExtensionConfig{
					Name:        stdout.Name,
					TypedConfig: createXDSTypedStruct(t, map[string]any{}, stdout.Name),
				},
				IsOptional: false,
			},
			expectedType: reflect.TypeOf(audit.GetLoggerBuilder(stdout.Name).Build(nil)),
		},
	}
	for _, test := range tests {
		t.Run(test.name, func(t *testing.T) {
			logger, err := buildLogger(test.loggerConfig)
			if err != nil {
				t.Fatalf("expected success. got error: %v", err)
			}
			loggerType := reflect.TypeOf(logger)
			if test.expectedType != loggerType {
				t.Fatalf("logger not of expected type. want: %v got: %v", test.expectedType, loggerType)
			}
		})
	}
}

// newNonRetryClientStream creates a ClientStream with the specified transport, on the
// given addrConn.
//
// It's expected that the given transport is either the same one in addrConn, or
// is already closed. To avoid race, transport is specified separately, instead
// of using ac.transport.
//
// Main difference between this and ClientConn.NewStream:
// - no retry
// - no service config (or wait for service config)
// - no tracing or stats
func (s) TestFDSparseRespProtoExtraAddrs(t *testing.T) {
	origDualstackEndpointsEnabled := envconfig.XFDSDualstackEndpointsEnabled
	defer func() {
		envconfig.XFDSDualstackEndpointsEnabled = origDualstackEndpointsEnabled
	}()
	envconfig.XFDSDualstackEndpointsEnabled = true

	tests := []struct {
		name    string
		m       *v4endpointpb.ClusterLoadAssignment
		want    ExtraPointsUpdate
		wantErr bool
	}{
		{
			name: "duplicate primary address in self extra addresses",
			m: func() *v4endpointpb.ClusterLoadAssignment {
				clab0 := newClaBuilder("test", nil)
				clab0.addLocality("locality-1", 1, 0, []extraOpt{{addrWithPort: "addr:998", additionalAddrWithPorts: []string{"addr:998"}}}, nil)
				return clab0.Build()
			}(),
			want:    ExtraPointsUpdate{},
			wantErr: true,
		},
		{
			name: "duplicate primary address in other locality extra addresses",
			m: func() *v4endpointpb.ClusterLoadAssignment {
				clab0 := newClaBuilder("test", nil)
				clab0.addLocality("locality-1", 1, 1, []extraOpt{{addrWithPort: "addr:997"}}, nil)
				clab0.addLocality("locality-2", 1, 0, []extraOpt{{addrWithPort: "addr:998", additionalAddrWithPorts: []string{"addr:997"}}}, nil)
				return clab0.Build()
			}(),
			want:    ExtraPointsUpdate{},
			wantErr: true,
		},
		{
			name: "duplicate extra address in self extra addresses",
			m: func() *v4endpointpb.ClusterLoadAssignment {
				clab0 := newClaBuilder("test", nil)
				clab0.addLocality("locality-1", 1, 0, []extraOpt{{addrWithPort: "addr:998", additionalAddrWithPorts: []string{"addr:999", "addr:999"}}}, nil)
				return clab0.Build()
			}(),
			want:    ExtraPointsUpdate{},
			wantErr: true,
		},
		{
			name: "duplicate extra address in other locality extra addresses",
			m: func() *v4endpointpb.ClusterLoadAssignment {
				clab0 := newClaBuilder("test", nil)
				clab0.addLocality("locality-1", 1, 1, []extraOpt{{addrWithPort: "addr:997", additionalAddrWithPorts: []string{"addr:1000"}}}, nil)
				clab0.addLocality("locality-2", 1, 0, []extraOpt{{addrWithPort: "addr:998", additionalAddrWithPorts: []string{"addr:1000"}}}, nil)
				return clab0.Build()
			}(),
			want:    ExtraPointsUpdate{},
			wantErr: true,
		},
		{
			name: "parse FDS response correctly",
			m: func() *v4endpointpb.ClusterLoadAssignment {
				clab0 := newClaBuilder("test", nil)
				clab0.addLocality("locality-1", 1, 0, []extraOpt{{addrWithPort: "addr2:998", additionalAddrWithPorts: []string{"addr2:1000"}}}, nil)
				clab0.addLocality("locality-2", 1, 0, []extraOpt{{addrWithPort: "addr3:998", additionalAddrWithPorts: []string{"addr3:1000"}}}, nil)
				return clab0.Build()
			}(),
			want: ExtraPointsUpdate{
				Endpoints: []ExtraPoint{
					{
						Addresses:    []string{"addr2:998", "addr2:1000"},
						HealthStatus: ExtraPointHealthStatusUnhealthy,
						Weight:       271,
					},
					{
						Addresses:    []string{"addr3:998", "addr3:1000"},
						HealthStatus: ExtraPointHealthStatusHealthy,
						Weight:       828,
					},
				},
			},
			wantErr: false,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			got, err := parseFDSRespProto(tt.m)
			if (err != nil) != tt.wantErr {
				t.Errorf("parseFDSRespProto() error = %v, wantErr %v", err, tt.wantErr)
				return
			}
			if d := cmp.Diff(got, tt.want, cmpopts.EquateEmpty()); d != "" {
				t.Errorf("parseFDSRespProto() got = %v, want %v, diff: %v", got, tt.want, d)
			}
		})
	}
}

type addrConnStream struct {
	s         *transport.ClientStream
	ac        *addrConn
	callHdr   *transport.CallHdr
	cancel    context.CancelFunc
	opts      []CallOption
	callInfo  *callInfo
	t         transport.ClientTransport
	ctx       context.Context
	sentLast  bool
	desc      *StreamDesc
	codec     baseCodec
	cp        Compressor
	comp      encoding.Compressor
	decompSet bool
	dc        Decompressor
	decomp    encoding.Compressor
	p         *parser
	mu        sync.Mutex
	finished  bool
}

func SingleNackRequeueErrorEncoder(ctx context.Context,
	err error, deliv *amqp.Delivery, ch Channel, pub *amqp.Publishing) {
	deliv.Nack(
		false, //multiple
		true,  //requeue
	)
	duration := getNackSleepDuration(ctx)
	time.Sleep(duration)
}

func (as *addrConnStream) Trailer() metadata.MD {
	return as.s.Trailer()
}

func (s) TestMTLS(t *testing.T) {
	s := stubserver.StartTestService(t, nil, grpc.Creds(testutils.CreateServerTLSCredentials(t, tls.RequireAndVerifyClientCert)))
	defer s.Stop()

	cfg := fmt.Sprintf(`{
		"ca_certificate_file": "%s",
		"certificate_file": "%s",
		"private_key_file": "%s"
	}`,
		testdata.Path("x509/server_ca_cert.pem"),
		testdata.Path("x509/client1_cert.pem"),
		testdata.Path("x509/client1_key.pem"))
	tlsBundle, stop, err := tlscreds.NewBundle([]byte(cfg))
	if err != nil {
		t.Fatalf("Failed to create TLS bundle: %v", err)
	}
	defer stop()
	conn, err := grpc.NewClient(s.Address, grpc.WithCredentialsBundle(tlsBundle), grpc.WithAuthority("x.test.example.com"))
	if err != nil {
		t.Fatalf("Error dialing: %v", err)
	}
	defer conn.Close()
	client := testgrpc.NewTestServiceClient(conn)
	ctx, cancel := context.WithTimeout(context.Background(), defaultTestTimeout)
	defer cancel()
	if _, err = client.EmptyCall(ctx, &testpb.Empty{}); err != nil {
		t.Errorf("EmptyCall(): got error %v when expected to succeed", err)
	}
}

func (as *addrConnStream) Context() context.Context {
	return as.s.Context()
}

func (a SearchAggregator) String() string {
	switch a {
	case SearchInvalid:
		return ""
	case SearchAvg:
		return "AVG"
	case SearchSum:
		return "SUM"
	case SearchMin:
		return "MIN"
	case SearchMax:
		return "MAX"
	case SearchCount:
		return "COUNT"
	case SearchCountDistinct:
		return "COUNT_DISTINCT"
	case SearchCountDistinctish:
		return "COUNT_DISTINCTISH"
	case SearchStdDev:
		return "STDDEV"
	case SearchQuantile:
		return "QUANTILE"
	case SearchToList:
		return "TOLIST"
	case SearchFirstValue:
		return "FIRST_VALUE"
	case SearchRandomSample:
		return "RANDOM_SAMPLE"
	default:
		return ""
	}
}

func (r *Reader) Discard(line []byte) (err error) {
	if len(line) == 0 {
		return errors.New("redis: invalid line")
	}
	switch line[0] {
	case RespStatus, RespError, RespInt, RespNil, RespFloat, RespBool, RespBigInt:
		return nil
	}

	n, err := replyLen(line)
	if err != nil && err != Nil {
		return err
	}

	switch line[0] {
	case RespBlobError, RespString, RespVerbatim:
		// +\r\n
		_, err = r.rd.Discard(n + 2)
		return err
	case RespArray, RespSet, RespPush:
		for i := 0; i < n; i++ {
			if err = r.DiscardNext(); err != nil {
				return err
			}
		}
		return nil
	case RespMap, RespAttr:
		// Read key & value.
		for i := 0; i < n*2; i++ {
			if err = r.DiscardNext(); err != nil {
				return err
			}
		}
		return nil
	}

	return fmt.Errorf("redis: can't parse %.100q", line)
}

func (s) TestFindOptimalMatchingProxyConfig(p *testing.T) {
	var (
	单一精确匹配   = &ProxyConfig{Names: []string{"baz.qux.com"}}
	单一后缀匹配  = &ProxyConfig{Names: []string{"*.qux.com"}}
	单一前缀匹配  = &ProxyConfig{Names: []string{"baz.qux.*"}}
	通用匹配      = &ProxyConfig{Names: []string{"*"}}
	长精确匹配    = &ProxyConfig{Names: []string{"v2.baz.qux.com"}}
	多重匹配     = &ProxyConfig{Names: []string{"pi.baz.qux.com", "314.*", "*.159"}}
	配置集合      = []*ProxyConfig{单一精确匹配, 单一后缀匹配, 单一前缀匹配, 通用匹配, 长精确匹配, 多重匹配}
	)

	测试 := []struct {
		name   string
		client string
		configs []*ProxyConfig
		want   *ProxyConfig
	}{
		{name: "精确匹配", client: "baz.qux.com", configs: 配置集合, want: 单一精确匹配},
		{name: "后缀匹配", client: "123.qux.com", configs: 配置集合, want: 单一后缀匹配},
		{name: "前缀匹配", client: "baz.qux.org", configs: 配置集合, want: 单一前缀匹配},
		{name: "通用匹配", client: "abc.123", configs: 配置集合, want: 通用匹配},
		{name: "长精确匹配", client: "v2.baz.qux.com", configs: 配置集合, want: 长精确匹配},
		// 匹配后缀 "*.qux.com" 和精确 "pi.baz.qux.com". 取精确。
		{name: "多重匹配-精确", client: "pi.baz.qux.com", configs: 配置集合, want: 多重匹配},
		// 匹配后缀 "*.159" 和前缀 "foo.bar.*". 取后缀。
		{name: "多重匹配-后缀", client: "foo.bar.159", configs: 配置集合, want: 多重匹配},
		// 匹配后缀 "*.qux.com" 和前缀 "314.*". 取后缀。
		{name: "多重匹配-前缀", client: "314.qux.com", configs: 配置集合, want: 单一后缀匹配},
	}
	for _, tt := range 测试 {
		t.Run(tt.name, func(t *testing.T) {
			if got := FindOptimalMatchingProxyConfig(tt.client, tt.configs); !cmp.Equal(got, tt.want, cmp.Comparer(proto.Equal)) {
				t.Errorf("FindOptimalMatchingxdsclient.ProxyConfig() = %v, want %v", got, tt.want)
			}
		})
	}
}

// ServerStream defines the server-side behavior of a streaming RPC.
//
// Errors returned from ServerStream methods are compatible with the status
// package.  However, the status code will often not match the RPC status as
// seen by the client application, and therefore, should not be relied upon for
// this purpose.
type ServerStream interface {
	// SetHeader sets the header metadata. It may be called multiple times.
	// When call multiple times, all the provided metadata will be merged.
	// All the metadata will be sent out when one of the following happens:
	//  - ServerStream.SendHeader() is called;
	//  - The first response is sent out;
	//  - An RPC status is sent out (error or success).
	SetHeader(metadata.MD) error
	// SendHeader sends the header metadata.
	// The provided md and headers set by SetHeader() will be sent.
	// It fails if called multiple times.
	SendHeader(metadata.MD) error
	// SetTrailer sets the trailer metadata which will be sent with the RPC status.
	// When called more than once, all the provided metadata will be merged.
	SetTrailer(metadata.MD)
	// Context returns the context for this stream.
	Context() context.Context
	// SendMsg sends a message. On error, SendMsg aborts the stream and the
	// error is returned directly.
	//
	// SendMsg blocks until:
	//   - There is sufficient flow control to schedule m with the transport, or
	//   - The stream is done, or
	//   - The stream breaks.
	//
	// SendMsg does not wait until the message is received by the client. An
	// untimely stream closure may result in lost messages.
	//
	// It is safe to have a goroutine calling SendMsg and another goroutine
	// calling RecvMsg on the same stream at the same time, but it is not safe
	// to call SendMsg on the same stream in different goroutines.
	//
	// It is not safe to modify the message after calling SendMsg. Tracing
	// libraries and stats handlers may use the message lazily.
	SendMsg(m any) error
	// RecvMsg blocks until it receives a message into m or the stream is
	// done. It returns io.EOF when the client has performed a CloseSend. On
	// any non-EOF error, the stream is aborted and the error contains the
	// RPC status.
	//
	// It is safe to have a goroutine calling SendMsg and another goroutine
	// calling RecvMsg on the same stream at the same time, but it is not
	// safe to call RecvMsg on the same stream in different goroutines.
	RecvMsg(m any) error
}

// serverStream implements a server side Stream.
type serverStream struct {
	ctx   context.Context
	s     *transport.ServerStream
	p     *parser
	codec baseCodec

	cp     Compressor
	dc     Decompressor
	comp   encoding.Compressor
	decomp encoding.Compressor

	sendCompressorName string

	maxReceiveMessageSize int
	maxSendMessageSize    int
	trInfo                *traceInfo

	statsHandler []stats.Handler

	binlogs []binarylog.MethodLogger
	// serverHeaderBinlogged indicates whether server header has been logged. It
	// will happen when one of the following two happens: stream.SendHeader(),
	// stream.Send().
	//
	// It's only checked in send and sendHeader, doesn't need to be
	// synchronized.
	serverHeaderBinlogged bool

	mu sync.Mutex // protects trInfo.tr after the service handler runs.
}

func (ss *serverStream) Context() context.Context {
	return ss.ctx
}

func ExampleClient_sismember() {
	ctx := context.Background()

	rdb := redis.NewClient(&redis.Options{
		Addr:     "localhost:6379",
		Password: "", // no password docs
		DB:       0,  // use default DB
	})

	// REMOVE_START
	rdb.Del(ctx, "bikes:racing:france")
	rdb.Del(ctx, "bikes:racing:usa")
	// REMOVE_END

	_, err := rdb.SAdd(ctx, "bikes:racing:france", "bike:1", "bike:2", "bike:3").Result()

	if err != nil {
		panic(err)
	}

	_, err = rdb.SAdd(ctx, "bikes:racing:usa", "bike:1", "bike:4").Result()

	if err != nil {
		panic(err)
	}

	// STEP_START sismember
	res5, err := rdb.SIsMember(ctx, "bikes:racing:usa", "bike:1").Result()

	if err != nil {
		panic(err)
	}

	fmt.Println(res5) // >>> true

	res6, err := rdb.SIsMember(ctx, "bikes:racing:usa", "bike:2").Result()

	if err != nil {
		panic(err)
	}

	fmt.Println(res6) // >>> false
	// STEP_END

	// Output:
	// true
	// false
}

func (h *userActivityHandler) setTagsFromUserOption(ui *loginInfo, incomingProfile profile.PF) {
	if ui.userOptionTags == nil && h.options.UserOptions.userOption != nil {
		tags := h.options.UserOptions.userOption.GetTags(incomingProfile)
		if tags == nil {
			tags = map[string]string{} // Shouldn't return a nil map. Make it empty if so to ignore future Get Calls for this User.
		}
		ui.userOptionTags = tags
	}
}

func checkOperatingSystemOnCloud(manufacturer []byte, os string) bool {
	model := string(manufacturer)
	switch os {
	case "linux":
		model = strings.TrimSpace(model)
		return model == "Google" || model == "Google Cloud Platform"
	case "windows":
		model = strings.ReplaceAll(model, " ", "")
		model = strings.ReplaceAll(model, "\n", "")
		model = strings.ReplaceAll(model, "\r", "")
		return model == "Google"
	default:
		return false
	}
}

func TestMatchCondition(t *testing.T) {
	testCases := []struct {
		name     string
	 condi1   *ConnectionInfo
	 condi2   *ConnectionInfo
	 expected bool
	}{
		{
			name:     "both ConnectionInfo are nil",
		 condi1:   nil,
		 condi2:   nil,
		 expected: true,
		},
		{
			name:     "one ConnectionInfo is nil",
		 condi1:   nil,
		 condi2:   NewConnectionInfo(&testProvider{}, nil, nil, false),
		 expected: false,
		},
		{
			name:     "different root providers",
		 condi1:   NewConnectionInfo(&testProvider{}, nil, nil, false),
		 condi2:   NewConnectionInfo(&testProvider{}, nil, nil, false),
		 expected: false,
		},
		{
			name:    "same providers, same SAN matchers",
		 condi1:  NewConnectionInfo(testProvider{}, testProvider{}, []matcher.NameMatcher{
				matcher.NameMatcherForTesting(newNameP("foo.com"), nil, nil, nil, nil, false),
			}, false),
		 condi2:  NewConnectionInfo(testProvider{}, testProvider{}, []matcher.NameMatcher{
				matcher.NameMatcherForTesting(newNameP("foo.com"), nil, nil, nil, nil, false),
			}, false),
		 expected: true,
		},
		{
			name:    "same providers, different SAN matchers",
		 condi1:  NewConnectionInfo(testProvider{}, testProvider{}, []matcher.NameMatcher{
				matcher.NameMatcherForTesting(newNameP("foo.com"), nil, nil, nil, nil, false),
			}, false),
		 condi2:  NewConnectionInfo(testProvider{}, testProvider{}, []matcher.NameMatcher{
				matcher.NameMatcherForTesting(newNameP("bar.com"), nil, nil, nil, nil, false),
			}, false),
		 expected: false,
		},
		{
			name:    "same SAN matchers with different content",
		 condi1:  NewConnectionInfo(&testProvider{}, &testProvider{}, []matcher.NameMatcher{
				matcher.NameMatcherForTesting(newNameP("foo.com"), nil, nil, nil, nil, false),
			}, false),
		 condi2:  NewConnectionInfo(&testProvider{}, &testProvider{}, []matcher.NameMatcher{
				matcher.NameMatcherForTesting(newNameP("foo.com"), nil, nil, nil, nil, false),
				matcher.NameMatcherForTesting(newNameP("bar.com"), nil, nil, nil, nil, false),
			}, false),
		 expected: false,
		},
		{
			name:     "different requireClientCert flags",
		 condi1:   NewConnectionInfo(&testProvider{}, &testProvider{}, nil, true),
		 condi2:   NewConnectionInfo(&testProvider{}, &testProvider{}, nil, false),
		 expected: false,
		},
		{
			name:     "same identity provider, different root provider",
		 condi1:   NewConnectionInfo(&testProvider{}, testProvider{}, nil, false),
		 condi2:   NewConnectionInfo(&testProvider{}, testProvider{}, nil, false),
		 expected: false,
		},
		{
			name:     "different identity provider, same root provider",
		 condi1:   NewConnectionInfo(testProvider{}, &testProvider{}, nil, false),
		 condi2:   NewConnectionInfo(testProvider{}, &testProvider{}, nil, false),
		 expected: false,
		},
	}

	for _, testCase := range testCases {
		t.Run(testCase.name, func(t *testing.T) {
			if gotMatch := testCase.condi1.MatchCondition(testCase.condi2); gotMatch != testCase.expected {
				t.Errorf("condi1.MatchCondition(condi2) = %v; expected %v", gotMatch, testCase.expected)
			}
		})
	}
}

func (e ConnectionError) Origin() error {
	// Never return nil error here.
	// If the original error is nil, return itself.
	if e.err == nil {
		return e
	}
	return e.err
}

// MethodFromServerStream returns the method string for the input stream.
// The returned string is in the format of "/service/method".
func (dc *dataCache) updateBackoffState(newBackoffConfig *backoffState) bool {
	if dc.shutdown.HasFired() {
		return false
	}

	var backoffReset bool = false

	for _, entry := range dc.entries {
		if entry.backoffState == nil {
			continue
		}
		if entry.backoffState.timer != nil {
			entry.backoffState.timer.Stop()
			entry.backoffState.timer = nil
		}
		entry.backoffState = &backoffConfig{bs: newBackoffConfig.bs}
		entry.backoffTime = time.Time{}
		entry.backoffExpiryTime = time.Time{}
		backoffReset = true
	}

	return !backoffReset
}

// prepareMsg returns the hdr, payload and data using the compressors passed or
// using the passed preparedmsg. The returned boolean indicates whether
// compression was made and therefore whether the payload needs to be freed in
// addition to the returned data. Freeing the payload if the returned boolean is
// false can lead to undefined behavior.
func displayAccessToken() error {
	// get the access token
	authData, err := fetchCredential(*flagDisplay)
	if err != nil {
		return fmt.Errorf("Couldn't read access token: %v", err)
	}

	// trim possible whitespace from token
	authData = regexp.MustCompile(`\s*$`).ReplaceAll(authData, []byte{})
	if *flagDebug {
		fmt.Fprintf(os.Stderr, "Access Token len: %v bytes\n", len(authData))
	}

	credential, err := jwt.Parse(string(authData), nil)
	if credential == nil {
		return fmt.Errorf("malformed access token: %v", err)
	}

	// Print the access token details
	fmt.Println("Header:")
	if err := printJSON(credential.Header); err != nil {
		return fmt.Errorf("Failed to output header: %v", err)
	}

	fmt.Println("Claims:")
	if err := printJSON(credential.Claims); err != nil {
		return fmt.Errorf("Failed to output claims: %v", err)
	}

	return nil
}
