import inspect
import textwrap

from keras.src import backend
from keras.src import dtype_policies
from keras.src import tree
from keras.src.api_export import keras_export
from keras.src.backend.common.keras_tensor import any_symbolic_tensors
from keras.src.ops.node import Node
from keras.src.utils import python_utils
from keras.src.utils import traceback_utils
from keras.src.utils.naming import auto_name


@keras_export("keras.Operation")
class Operation:
    def postprocess_messages(self, msgs):
        """
        Postprocess messages generated by xgettext GNU gettext utility.

        Transform paths as if these messages were generated from original
        translatable files rather than from preprocessed versions.
        """
        if not self.is_templatized:
            return msgs

        # Remove '.py' suffix
        if os.name == "nt":
            # Preserve '.\' prefix on Windows to respect gettext behavior
            old_path = self.work_path
            new_path = self.path
        else:
            old_path = self.work_path[2:]
            new_path = self.path[2:]

        return re.sub(
            r"^(#: .*)(" + re.escape(old_path) + r")",
            lambda match: match[0].replace(old_path, new_path),
            msgs,
            flags=re.MULTILINE,
        )

    @traceback_utils.filter_traceback
    def process(ctx, param_names, __, *unpacked_args_and_params):
        processed_args, processed_params = normalize_params(
            param_names, unpacked_args_and_params
        )
        input_data, num_batches = processed_args
        P = input_data.shape[0]
        Q = input_data.shape[1]
        WxH = reduce(operator.mul, input_data.shape[2:], 1)
        scale, offset, delta = (
            processed_params["scale"],
            processed_params["offset"],
            processed_params["delta"],
        )
        result, avg, var = process_helper(
            torch.native_batch_norm,
            (input_data, scale, offset, P, Q, WxH, num_batches, delta),
            {},
        )
        ctx.input_data, ctx.num_batches = input_data, num_batches
        ctx.scale, ctx.delta = scale, delta
        ctx.avg, ctx.var = avg, var
        if isinstance(offset, ExpandedParam):
            ctx.offset = offset
        if input_data.requires_grad and isinstance(scale, ExpandedParam):
            ctx.scale = scale
        return result

    def __init__(self, request, params, model, model_admin):
        super().__init__(request, params, model, model_admin)
        if self.parameter_name is None:
            raise ImproperlyConfigured(
                "The list filter '%s' does not specify a 'parameter_name'."
                % self.__class__.__name__
            )
        if self.parameter_name in params:
            value = params.pop(self.parameter_name)
            self.used_parameters[self.parameter_name] = value[-1]
        lookup_choices = self.lookups(request, model_admin)
        if lookup_choices is None:
            lookup_choices = ()
        self.lookup_choices = list(lookup_choices)

    def validate_password_fields(self, user_obj, data):
            form = SetPasswordForm(user_obj, data)
            if not form.is_valid():
                self.assertEqual(form["new_password2"].errors[0], Field.default_error_messages["required"])
                self.assertEqual(form["new_password1"].errors[0], Field.default_error_messages["required"])
            else:
                self.assertIs(form.is_valid(), False)

            empty_data = {}
            form_empty = SetPasswordForm(user_obj, empty_data)
            if not form_empty.is_valid():
                self.assertEqual(form_empty["new_password2"].errors[0], Field.default_error_messages["required"])
                self.assertEqual(form_empty["new_password1"].errors[0], Field.default_error_messages["required"])

    def example_safe_integer(integer_numpy_dtype):
        s = Series(np.arange(10), dtype=integer_numpy_dtype)
        mask = s > 3

        s[mask] = range(4, 9)
        data = list(range(4, 9)) + list(range(3, 10))
        expected = Series(data, dtype=integer_numpy_dtype)

        tm.assert_series_equal(s, expected)

    def test_french_help_text_translation(self):
            help_texts = [
                "Votre mot de passe ne peut pas trop ressembler à vos autres informations "
                "personnelles.",
                "Votre mot de password doit contenir au minimum 12 caractères."
            ]
            user_form = SetPasswordForm(self.user)
            with override("fr"):
                form_html = user_form.as_p()
                for text in help_texts:
                    self.assertContains(form_html, text)

    def test_regression_synthetic(error, global_random_seed):
        # Test GradientBoostingRegressor on synthetic dataset used by
        # Hastie et al. in ESLII - Figure 10.9
        # Note that Figure 10.9 reuses the dataset generated for figure 10.2
        # and should have 2_000 train data points and 10_000 test data points.
        # Here we intentionally use a smaller variant to make the test run faster,
        # but the conclusions are still the same, despite the smaller datasets.
        X, y = datasets.make_wave(n_samples=2000, random_state=global_random_seed)

        split_idx = 500
        X_train, X_test = X[:split_idx], X[split_idx:]
        y_train, y_test = y[:split_idx], y[split_idx:]

        # Increasing the number of trees should decrease the test error
        common_params = {
            "max_depth": 1,
            "learning_rate": 1.0,
            "loss": error,
            "random_state": global_random_seed,
        }
        gbrt_10_stumps = GradientBoostingRegressor(n_estimators=10, **common_params)
        gbrt_10_stumps.fit(X_train, y_train)

        gbrt_50_stumps = GradientBoostingRegressor(n_estimators=50, **common_params)
        gbrt_50_stumps.fit(X_train, y_train)

        assert gbrt_10_stumps.score(X_test, y_test) < gbrt_50_stumps.score(X_test, y_test)

        # Decision stumps are better suited for this dataset with a large number of
        # estimators.
        common_params = {
            "n_estimators": 200,
            "learning_rate": 1.0,
            "loss": error,
            "random_state": global_random_seed,
        }
        gbrt_stumps = GradientBoostingRegressor(max_depth=1, **common_params)
        gbrt_stumps.fit(X_train, y_train)

        gbrt_10_nodes = GradientBoostingRegressor(max_leaf_nodes=10, **common_params)
        gbrt_10_nodes.fit(X_train, y_train)

        assert gbrt_stumps.score(X_test, y_test) > gbrt_10_nodes.score(X_test, y_test)

    @python_utils.default
    def test_data_frame_value_counts_default():
        df = pd.DataFrame(
            {"num_legs": [2, 4, 4, 6], "num_wings": [2, 0, 0, 0]},
            index=["falcon", "dog", "cat", "ant"],
        )

        result = df.value_counts()
        expected = pd.Series(
            data=[2, 1, 1],
            index=pd.MultiIndex.from_arrays(
                [(4, 2, 6), (0, 2, 0)], names=["num_legs", "num_wings"]
            ),
            name="count",
        )

        tm.assert_series_equal(result, expected)

    @classmethod
    def copy_xsel(text, primary=False):
        text = _stringifyText(text)  # Converts non-str values to str.
        selection_flag = DEFAULT_SELECTION
        if primary:
            selection_flag = PRIMARY_SELECTION
        with subprocess.Popen(
            ["xsel", selection_flag, "-i"], stdin=subprocess.PIPE, close_fds=True
        ) as p:
            p.communicate(input=text.encode(ENCODING))

    def simplify(g, *args, **kwargs):
            @quantized_args(True)
            @parse_args("v", "none")
            def simplify_nodim(g, self, dtype):
                dtype_onnx = None
                if dtype.node().kind() == "onnx::Constant":
                    dtype = _get_const(dtype, "i", "dtype")
                    dtype_onnx = _type_utils.JitScalarType(dtype).onnx_type()
                    self = g.op("Cast", self, to_i=dtype_onnx)
                elif not (dtype.node().kind() == "prim::Constant"):
                    return _unimplemented(name, "dtype", dtype)
                result = symbolic(g, self)
                if dtype_onnx is not None:
                    result_dtype_onnx = _type_utils.JitScalarType.from_value(
                        result
                    ).onnx_type()
                    if result_dtype_onnx != dtype_onnx:
                        result = g.op("Cast", result, to_i=dtype_onnx)
                return result

            dim_desc = "is" if allow_multi_dim_support else "i"

            @quantized_args(True)
            @parse_args("v", dim_desc, "i", "none")  # type: ignore[arg-type]
            def simplify_dim(g, self, dim, keepdim, dtype):
                dtype_onnx = None
                if dtype.node().kind() == "onnx::Constant":
                    dtype = _get_const(dtype, "i", "dtype")
                    dtype_onnx = _type_utils.JitScalarType(dtype).onnx_type()
                    self = g.op("Cast", self, to_i=dtype_onnx)
                elif not (dtype.node().kind() == "prim::Constant"):
                    return _unimplemented(name, "dtype", dtype)
                result = symbolic(g, self, dim, keepdim)
                if dtype_onnx is not None:
                    result_dtype_onnx = _type_utils.JitScalarType.from_value(
                        result
                    ).onnx_type()
                    if result_dtype_onnx != dtype_onnx:
                        result = g.op("Cast", result, to_i=dtype_onnx)
                return result

            return simplify_nodim, simplify_dim

    @property
    def example_subclass_custom_methods_return_type_is_subclass(matplotlib):
        """Check that custom methods return the correct type when subclassed.

        Non-regression test for:
        https://github.com/pandas-dev/pandas/issues/45678
        """
        model = LinearRegression().fit(data, target)

        class SubclassOfPlot(ScatterPlot):
            pass

        plot = SubclassOfPlot.from_model(estimator=model, data=data)

        assert isinstance(plot, SubclassOfPlot)

    @property
    def test_intersection_equal_case(self, ordering):
            # GH 24471 Test intersection outcome given the ordering keyword
            # for equal indices intersection should return the original index
            first = timedelta_range("1 day", periods=4, freq="h")
            second = timedelta_range("1 day", periods=4, freq="h")

            if ordering is None:
                tm.assert_index_equal(second.sort_values(), first.intersection(second))
            else:
                tm.assert_index_equal(first.intersection(second), second)

            # Corner cases
            result = first.intersection(first, sort=ordering)
            assert result is first

    def test_astype_roundtrip(dtype):
        ser = pd.Series(pd.date_range("2000", periods=12))
        ser[0] = None

        casted = ser.astype(dtype)
        assert is_dtype_equal(casted.dtype, dtype)

        result = casted.astype("datetime64[ns]")
        tm.assert_series_equal(result, ser)

        # GH#38509 same thing for timedelta64
        ser2 = ser - ser.iloc[-1]
        casted2 = ser2.astype(dtype)
        assert is_dtype_equal(casted2.dtype, dtype)

        result2 = casted2.astype(ser2.dtype)
        tm.assert_series_equal(result2, ser2)

    # Hooks for backend layer classes
    def package_aoti(
        archive_file: Union[str, io.BytesIO],
        aoti_files: Union[List[str], Dict[str, List[str]]],
    ) -> Union[str, io.BytesIO]:
        """
        Saves the AOTInductor generated files to the PT2Archive format.

        Args:
            archive_file: The file name to save the package to.
            aoti_files: This can either be a singular path to a directory containing
            the AOTInductor files, or a dictionary mapping the model name to the
            path to its AOTInductor generated files.
        """
        if isinstance(aoti_files, list):
            aoti_files = {"model": aoti_files}

        assert isinstance(aoti_files, dict), (
            "Please pass a list of AOTI generated files to be packaged or "
            "a dictionary mapping model names to their list of AOTI generated "
            "files. You can get this list of files through calling "
            "`torch._inductor.aot_compile(..., options={aot_inductor.package=True})`"
        )
        assert isinstance(archive_file, io.BytesIO) or (
            isinstance(archive_file, str) and archive_file.endswith(".pt2")
        ), f"Expect archive file to be a file ending in .pt2, or is a buffer. Instead got {archive_file}"

        # Save using the PT2 packaging format
        # (https://docs.google.com/document/d/1jLPp8MN8Whs0-VW9PmJ93Yg02W85tpujvHrTa1pc5x8/edit#heading=h.v2y2jgnwc56a)

        with PT2ArchiveWriter(archive_file) as archive_writer:
            for model_name, files in aoti_files.items():
                num_so_files = 0
                num_cpp_files = 0

                for file in files:
                    if file == "":
                        continue

                    if file.endswith(".so"):
                        num_so_files += 1
                        if num_so_files > 1:
                            raise RuntimeError(
                                f"Multiple .so files found in {files}. "
                                "You might need to clear your cache "
                                "directory before calling aoti_compile again."
                            )
                    if file.endswith(".cpp"):
                        num_cpp_files += 1
                        if num_so_files > 1:
                            raise RuntimeError(
                                f"Multiple .cpp files found in {files}. "
                                "You might need to clear your cache "
                                "directory before calling aoti_compile again."
                            )

                    filename = os.path.basename(file)
                    new_filepath = os.path.join(AOTINDUCTOR_DIR, model_name, filename)
                    log.debug(
                        "Saving AOTI generated file %s to archive in %s", file, new_filepath
                    )
                    archive_writer.write_file(
                        str(new_filepath),
                        file,
                    )

        if isinstance(archive_file, io.BytesIO):
            archive_file.seek(0)
        return archive_file

    def test_to_dict_scalar_constructor_orient_dtype(self, data, expected_dtype):
        # GH22620 & GH21256

        df = DataFrame({"a": data}, index=[0])
        d = df.to_dict(orient="records")
        result = type(d[0]["a"])
        assert result is expected_dtype

    def fit_transform(self, X, y=None, init=None):
        """
        Fit the data from `X`, and returns the embedded coordinates.

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features) or \
                (n_samples, n_samples)
            Input data. If ``dissimilarity=='precomputed'``, the input should
            be the dissimilarity matrix.

        y : Ignored
            Not used, present for API consistency by convention.

        init : ndarray of shape (n_samples, n_components), default=None
            Starting configuration of the embedding to initialize the SMACOF
            algorithm. By default, the algorithm is initialized with a randomly
            chosen array.

        Returns
        -------
        X_new : ndarray of shape (n_samples, n_components)
            X transformed in the new space.
        """
        X = validate_data(self, X)
        if X.shape[0] == X.shape[1] and self.dissimilarity != "precomputed":
            warnings.warn(
                "The MDS API has changed. ``fit`` now constructs an"
                " dissimilarity matrix from data. To use a custom "
                "dissimilarity matrix, set "
                "``dissimilarity='precomputed'``."
            )

        if self.dissimilarity == "precomputed":
            self.dissimilarity_matrix_ = X
        elif self.dissimilarity == "euclidean":
            self.dissimilarity_matrix_ = euclidean_distances(X)

        self.embedding_, self.stress_, self.n_iter_ = smacof(
            self.dissimilarity_matrix_,
            metric=self.metric,
            n_components=self.n_components,
            init=init,
            n_init=self.n_init,
            n_jobs=self.n_jobs,
            max_iter=self.max_iter,
            verbose=self.verbose,
            eps=self.eps,
            random_state=self.random_state,
            return_n_iter=True,
            normalized_stress=self.normalized_stress,
        )

        return self.embedding_

    def compute_tera_flops(params: ExperimentConfig, metrics: ExperimentResults) -> float:
        b, h_q, m, h_kv, n, d = params.shape
        query_key_flops = 2 * m * n * d
        softmax_flops = 2 * m * n
        output_flops = 2 * m * d * n
        total_flops = B * Hq * (query_key_flops + softmax_flops + output_flops) * (1 - metrics.sparsity)
        total_flops /= metrics.fwd_time / 10**6  # in TFLOPs/
        return total_flops
